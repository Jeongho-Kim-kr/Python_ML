{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 문장 vectorize해서 이용하는 모듈 원리\n",
    "## 직접 만든 예시\n",
    "이해 안될시 boostcamp 영상 보기\n",
    "https://www.boostcourse.org/ai222/lecture/253408?isDesc=false"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "# 파일 불러오기\n",
    "import os\n",
    "\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dir_name = 'news_data'\n",
    "    file_list = get_file_list(dir_name)\n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list] # os.path.join: 이걸로 해야 os에 상관없이 자동으로 상대 path를 생성해 join(os마다 구분이 다름 맥\\, 윈도우/)\n",
    "\n",
    "\n",
    "print(len(file_list)) # 파일 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일별로 내용 읽기\n",
    "def get_contents(file_list):\n",
    "    y_class = []\n",
    "    x_text = []\n",
    "    class_dict = {\n",
    "        1: '0', 2: '0', 3: '0', 4: '0', 5: '1', 6: '1', 7: '1', 8: '1'} # 뉴스 내용의 카테고리\n",
    "\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name, 'r', encoding='cp979')\n",
    "            category = int(file_name.split(os.sep)[1].split('-')[0]) # os.sep os에 맞는 /나 \\\n",
    "            y_class.append(class_dict[category]) # 80개 카테고리\n",
    "            x_text.append(f.read()) # 80개 문서\n",
    "            f.close()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'imyours'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Corpus 만들기 + 단어별 index 생성하기\n",
    "def get_cleaned_text(text): # 의미없는 문장보호 등은 제거하기\n",
    "    import re\n",
    "    text = re.sub('\\W+','', text.lower())\n",
    "    return text\n",
    "\n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text] # 단어별로 스플릿\n",
    "    cleaned_words = [get_cleaned_text(word) for words in text for word in words] # 1차원\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(cleaned_words)):\n",
    "        corpus_dict[v] = i\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "get_cleaned_text(\"I'm yours\") # 예시"
   ]
  },
  {
   "source": [
    "문서 내에서 단어 별로 corpus를(좌표) 할당"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서별로 Bag of words vector 생성\n",
    "def get_count_vector(text, corpus):\n",
    "    text = [sentence.split() for sentence in text] # 다큐먼트를 스플릿\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words] for words in text] # 2차원\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "\n",
    "\n",
    "    for i, text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "    return X_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교하기\n",
    "import math\n",
    "def get_cosine_similarity(v1,v2):\n",
    "    # compute cosine similarity of v1 to v2: (v1 dot v2) / (||v1|| * ||v2||}\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy / math.sqrt(sumxx*xumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교결과 정리하기\n",
    "def get_similarity_score(X_vector, source):\n",
    "    source_vector = X_vector[source]\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(get_cosine_similarity(source_vector, target_vector))\n",
    "    return similarity_list\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n):\n",
    "    import operator\n",
    "    x = {i:v for i, v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "    return list(reversed(sorted_x))[1:n+1]"
   ]
  },
  {
   "source": [
    "## 위 내용들로 만든 최종 내용\n",
    "결과 0.5보단 크다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of words : 4024\n",
      "0.6950000000000001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)\n",
    "\n",
    "def get_conetents(file_list):\n",
    "    y_class = []\n",
    "    X_text = []\n",
    "    class_dict = {\n",
    "        1: \"0\", 2: \"0\", 3:\"0\", 4:\"0\", 5:\"1\", 6:\"1\", 7:\"1\", 8:\"1\"}\n",
    "\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name, \"r\",  encoding=\"cp949\")\n",
    "            category = int(file_name.split(os.sep)[1].split(\"_\")[0])\n",
    "            y_class.append(class_dict[category])\n",
    "            X_text.append(f.read())\n",
    "            f.close()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class\n",
    "\n",
    "\n",
    "\n",
    "def get_cleaned_text(text):\n",
    "    import re\n",
    "    text = re.sub('\\W+','', text.lower() )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    cleaned_words = [get_cleaned_text(word) for words in text for word in words]\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(cleaned_words)):\n",
    "        corpus_dict[v] = i\n",
    "    return corpus_dict\n",
    "\n",
    "\n",
    "def get_count_vector(text, corpus):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words] for words in text]\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "\n",
    "    for i, text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "    return X_vector\n",
    "\n",
    "import math\n",
    "def get_cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def get_similarity_score(X_vector, source):\n",
    "    source_vector = X_vector[source]\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(\n",
    "            get_cosine_similarity(source_vector, target_vector))\n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n):\n",
    "    import operator\n",
    "    x = {i:v for i, v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "\n",
    "    return list(reversed(sorted_x))[1:n+1]\n",
    "\n",
    "def get_accuracy(similarity_list, y_class, source_news):\n",
    "    source_class = y_class[source_news]\n",
    "\n",
    "    return sum([source_class == y_class[i[0]] for i in similarity_list]) / len(similarity_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dir_name = \"news_data\"\n",
    "    file_list = get_file_list(dir_name)\n",
    "    file_list = [os.path.join(dir_name, file_name) for file_name in file_list]\n",
    "\n",
    "    X_text, y_class = get_conetents(file_list)\n",
    "\n",
    "    corpus = get_corpus_dict(X_text)\n",
    "    print(\"Number of words : {0}\".format(len(corpus)))\n",
    "    X_vector = get_count_vector(X_text, corpus)\n",
    "    source_number = 10\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(80):\n",
    "        source_number = i\n",
    "\n",
    "        similarity_score = get_similarity_score(X_vector, source_number)\n",
    "        similarity_news = get_top_n_similarity_news(similarity_score, 10)\n",
    "        accuracy_score = get_accuracy(similarity_news, y_class, source_number)\n",
    "        result.append(accuracy_score)\n",
    "    print(sum(result) / 80)"
   ]
  },
  {
   "source": [
    "## 위 원리로 실제 사용하는 패키지"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n [0 1 0 1 0 2 1 0 1]\n [1 0 0 0 1 0 1 1 0]\n [0 1 1 1 0 0 1 0 1]]\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray()) # 아래 단어 특성에 따라 vectorize 된 문장\n",
    "print(vectorizer.get_feature_names()) # 벡터에 해당되는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}